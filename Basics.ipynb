{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GREEDY ALGORITHM - traverse the max of current state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAZE = {\n",
    "        12: {\n",
    "            9: {\n",
    "                10: {\n",
    "                  100: 'End'      \n",
    "                }\n",
    "                }\n",
    "            },\n",
    "        12: {\n",
    "            10: {\n",
    "                55: {\n",
    "                  5: 'End'      \n",
    "                }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "MAZE2 = {\n",
    "        4: {\n",
    "            -100: {\n",
    "                4: 'End'\n",
    "                }\n",
    "            },\n",
    "        3: {\n",
    "            100: {\n",
    "                4: 'End'\n",
    "                }\n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedypolicy(current_state, total_reward = 0):\n",
    "    if (not isinstance(current_state, dict)):\n",
    "        print(\"Finished game with total reward of {}\".format(total_reward))\n",
    "    else:\n",
    "        new_state = max(current_state.keys())\n",
    "    \n",
    "        print(\"Taking action to get to state {}\".format(new_state))\n",
    "\n",
    "        policy(current_state[new_state], total_reward + new_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taking action to get to state 12\n",
      "Taking action to get to state 6\n",
      "Taking action to get to state 5\n",
      "Finished game with total reward of 23\n"
     ]
    }
   ],
   "source": [
    "greedypolicy(MAZE)\n",
    "\n",
    "#stupid algo since it just checks the first state reward for traversal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backwards - apply policy to the backwards MAZE in order to find max of last reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flat_map(array):\n",
    "    new_array = []\n",
    "\n",
    "    for a in array:\n",
    "        if isinstance(a, list):\n",
    "            new_array += flat_map(a)\n",
    "        else:\n",
    "            new_array.append(a)\n",
    "\n",
    "    return new_array\n",
    "\n",
    "def create_dict(flat_array):\n",
    "    head, *tail = flat_array\n",
    "\n",
    "    if len(tail) == 1:\n",
    "        return {head: tail[0]}\n",
    "    else:\n",
    "        return {head: create_dict(tail)}\n",
    "\n",
    "def invert_dict(dictionary, stack=None):\n",
    "    if not stack: stack = []\n",
    "\n",
    "    if (not isinstance(dictionary, dict)):\n",
    "        return dictionary\n",
    "\n",
    "    for k, v in dictionary.items():\n",
    "        stack.append([invert_dict(v), k])\n",
    "\n",
    "    return stack\n",
    "\n",
    "def create_new_maze(dictionary):\n",
    "    new_maze = {}\n",
    "    for path in invert_dict(dictionary):\n",
    "        new_maze.update(create_dict(flat_map(path)[1:]))\n",
    "\n",
    "    return new_maze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backpolicy(current_state):\n",
    "    upside_down_maze = create_new_maze(current_state)\n",
    "\n",
    "    states = []\n",
    "    while (isinstance(upside_down_maze, dict)):\n",
    "        new_state = max(upside_down_maze.keys())\n",
    "        states = [new_state] + states\n",
    "        upside_down_maze = upside_down_maze[new_state]\n",
    "\n",
    "    states = [upside_down_maze] + states\n",
    "\n",
    "    total_reward = 0\n",
    "    for s in states:\n",
    "        total_reward += s\n",
    "        print(\"Tacking action to get to state {}\".format(s))\n",
    "\n",
    "    print(\"Finished game with total reward of {}\".format(total_reward))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tacking action to get to state -99\n",
      "Tacking action to get to state 3\n",
      "Tacking action to get to state 4\n",
      "Tacking action to get to state 7\n",
      "Finished game with total reward of -85\n"
     ]
    }
   ],
   "source": [
    "backpolicy(MAZE)\n",
    "\n",
    "#stupid algo since it just checks the last state reward for traversal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bellman - apply discounted future reward equation as maximizing criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# v_i = r_i + gamma * v_{i+1}\n",
    "\n",
    "# v_i = r_i + gamma (r_{i+1} + gamma ........\n",
    "\n",
    "def discounted_reward(current_state, gamma = 0.9):\n",
    "    if (isinstance(current_state, dict)):\n",
    "        return sum([k + gamma * discounted_reward(v) for k,v in current_state.items()])\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def bellmanpolicy(current_state, total_reward = 0, gamma = 0.9):\n",
    "    if (not isinstance(current_state, dict)):\n",
    "        print(\"Finished the game with a total reward of {}\".format(total_reward))\n",
    "    else:\n",
    "        bellman_maze = {(k + gamma * discounted_reward(v), k): v for k,v in current_state.items()}\n",
    "\n",
    "        new_state = max(bellman_maze.keys())\n",
    "\n",
    "        print(\"Taking action to get to state {} ({})\".format(new_state[1], new_state[0]))\n",
    "\n",
    "        policy(bellman_maze[new_state], total_reward + new_state[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taking action to get to state 12 (69.19500000000001)\n",
      "Taking action to get to state 10\n",
      "Taking action to get to state 55\n",
      "Taking action to get to state 5\n",
      "Finished game with total reward of 82\n"
     ]
    }
   ],
   "source": [
    "bellmanpolicy(MAZE)\n",
    "\n",
    "# better than before. it selects the best long term path even if the first or last state rewards were high or equal\n",
    "# however it still doesnt take the optimal path if rewards are higher further down the line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
